{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81737e6c",
   "metadata": {},
   "source": [
    "# Data Modeling - Diabetes Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5d704f",
   "metadata": {},
   "source": [
    "**Authors:**  \n",
    "Filip Kobus, Łukasz Jarzęcki, Paweł Skierkowski  \n",
    "**Date:** 23.01.25  \n",
    "**Team 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92625f41",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Lasso, LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "\n",
    "sys.path.append(str(pathlib.Path.cwd().parent / 'src'))\n",
    "\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83db293",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce9b1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv('diabetes_dataset.csv').drop(['diabetes_stage'], axis=1)\n",
    "\n",
    "NUM_COLS = [\n",
    "    'age',\n",
    "    'alcohol_consumption_per_week',\n",
    "    'physical_activity_minutes_per_week',\n",
    "    'diet_score',\n",
    "    'sleep_hours_per_day',\n",
    "    'screen_time_hours_per_day',\n",
    "    'bmi',\n",
    "    'waist_to_hip_ratio',\n",
    "    'systolic_bp',\n",
    "    'diastolic_bp',\n",
    "    'heart_rate',\n",
    "    'cholesterol_total',\n",
    "    'hdl_cholesterol',\n",
    "    'ldl_cholesterol',\n",
    "    'triglycerides',\n",
    "    'glucose_fasting',\n",
    "    'glucose_postprandial',\n",
    "    'insulin_level',\n",
    "    'hba1c',\n",
    "]\n",
    "\n",
    "CAT_COLS = [\n",
    "    'family_history_diabetes',\n",
    "    'hypertension_history',\n",
    "    'cardiovascular_history',\n",
    "]\n",
    "\n",
    "ORDINAL_ENCODER_COLS = ['education_level', 'income_level', 'smoking_status']\n",
    "ORDINAL_ENCODER_CATS = [\n",
    "    ['No formal', 'Highschool', 'Graduate', 'Postgraduate'],\n",
    "    ['Low', 'Lower-Middle', 'Middle', 'Upper-Middle', 'High'],\n",
    "    ['Never', 'Former', 'Current'],\n",
    "]\n",
    "ONEHOT_ENCODER_COLS = ['employment_status', 'gender', 'ethnicity']\n",
    "\n",
    "\n",
    "def read_data(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path).drop(['diabetes_stage', 'diagnosed_diabetes'], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_preprocessor(\n",
    "    num_columns: list[str],\n",
    "    cat_columns: list[str],\n",
    "    ordinal_enc_columns: list[str],\n",
    "    ordinal_enc_categories: list[list[str]],\n",
    "    one_hot_enc_columns: list[str],\n",
    ") -> ColumnTransformer:\n",
    "    preprocessor = ColumnTransformer(\n",
    "        [\n",
    "            (\n",
    "                'num_pipeline',\n",
    "                Pipeline(\n",
    "                    [\n",
    "                        ('median_imputer', SimpleImputer(strategy='median')),\n",
    "                        ('scaler', StandardScaler()),\n",
    "                    ]\n",
    "                ),\n",
    "                num_columns,\n",
    "            ),\n",
    "            (\n",
    "                'cat_pipeline',\n",
    "                Pipeline(\n",
    "                    [\n",
    "                        (\n",
    "                            'most_frequent_imputer',\n",
    "                            SimpleImputer(strategy='most_frequent'),\n",
    "                        ),\n",
    "                    ]\n",
    "                ),\n",
    "                cat_columns,\n",
    "            ),\n",
    "            (\n",
    "                'ord_pipeline',\n",
    "                Pipeline(\n",
    "                    [\n",
    "                        (\n",
    "                            'most_frequent_imputer',\n",
    "                            SimpleImputer(strategy='most_frequent'),\n",
    "                        ),\n",
    "                        (\n",
    "                            'ordinal_encoder',\n",
    "                            OrdinalEncoder(categories=ordinal_enc_categories),\n",
    "                        ),\n",
    "                    ]\n",
    "                ),\n",
    "                ordinal_enc_columns,\n",
    "            ),\n",
    "            (\n",
    "                'onehot_pipeline',\n",
    "                Pipeline(\n",
    "                    [\n",
    "                        (\n",
    "                            'most_frequent_imputer',\n",
    "                            SimpleImputer(strategy='most_frequent'),\n",
    "                        ),\n",
    "                        (\n",
    "                            'ordinal_encoder',\n",
    "                            OneHotEncoder(\n",
    "                                drop='first',\n",
    "                                sparse_output=False,\n",
    "                                handle_unknown='ignore',\n",
    "                            ),\n",
    "                        ),\n",
    "                    ]\n",
    "                ),\n",
    "                one_hot_enc_columns,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "def remove_outliers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    diabetes = df\n",
    "    # blood pressure\n",
    "    impossible_bp = diabetes[\n",
    "        (diabetes['systolic_bp'] <= diabetes['diastolic_bp'])\n",
    "        | (diabetes['systolic_bp'] < 60)\n",
    "        | (diabetes['systolic_bp'] > 300)\n",
    "    ]\n",
    "\n",
    "    # cholesterol\n",
    "    cholesterol_error = diabetes[\n",
    "        diabetes['cholesterol_total']\n",
    "        < (diabetes['hdl_cholesterol'] + diabetes['ldl_cholesterol']) - 20\n",
    "    ]\n",
    "\n",
    "    # screen time, sleep hours, age\n",
    "    sleep_error = diabetes[\n",
    "        (diabetes['sleep_hours_per_day'] <= 0) | (diabetes['sleep_hours_per_day'] > 24)\n",
    "    ]\n",
    "\n",
    "    age_error = diabetes[(diabetes['age'] < 0) | (diabetes['age'] > 110)]\n",
    "\n",
    "    screentime_error = diabetes[\n",
    "        (diabetes['screen_time_hours_per_day'] < 0)\n",
    "        | (diabetes['screen_time_hours_per_day'] > 24)\n",
    "    ]\n",
    "\n",
    "    # WHR (Waist to hip ratio)\n",
    "    whr_error = diabetes[\n",
    "        (diabetes['waist_to_hip_ratio'] < 0.5) | (diabetes['waist_to_hip_ratio'] > 2.5)\n",
    "    ]\n",
    "\n",
    "    index_to_drop = (\n",
    "        set(impossible_bp.index)\n",
    "        | set(cholesterol_error.index)\n",
    "        | set(sleep_error.index)\n",
    "        | set(age_error.index)\n",
    "        | set(screentime_error.index)\n",
    "        | set(whr_error.index)\n",
    "    )\n",
    "\n",
    "    diabetes_clean = diabetes.drop(index=list(index_to_drop)).copy()\n",
    "\n",
    "    return diabetes_clean\n",
    "\n",
    "\n",
    "def split_data(df: pd.DataFrame) -> tuple:\n",
    "    target = 'diabetes_risk_score'\n",
    "    test_size = 0.2\n",
    "    random_state = 42\n",
    "\n",
    "    y = df[target]\n",
    "    X = df.drop(target, axis=1)\n",
    "\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "\n",
    "def format_col_names(cols: list[str]) -> list[str]:\n",
    "    new_cols = []\n",
    "    for col in cols:\n",
    "        new_cols.append(col.split('__')[1])\n",
    "    return new_cols\n",
    "\n",
    "\n",
    "def save_data(\n",
    "    X_train: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    y_test: pd.Series,\n",
    "):\n",
    "    X_train.to_csv('X_train.csv', index=False)\n",
    "    X_test.to_csv('X_test.csv', index=False)\n",
    "    y_train.to_csv('y_train.csv', index=False)\n",
    "    y_test.to_csv('y_test.csv', index=False)\n",
    "\n",
    "\n",
    "def process_features(\n",
    "    preprocessor: ColumnTransformer, X_train: pd.DataFrame, X_test: pd.DataFrame\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    X_train_pre = preprocessor.fit_transform(X_train)\n",
    "    X_test_pre = preprocessor.transform(X_test)\n",
    "    feature_names = format_col_names(preprocessor.get_feature_names_out())\n",
    "\n",
    "    X_train_pre_df = pd.DataFrame(X_train_pre, columns=feature_names)\n",
    "    X_test_pre_df = pd.DataFrame(X_test_pre, columns=feature_names)\n",
    "\n",
    "    return X_train_pre_df, X_test_pre_df\n",
    "\n",
    "\n",
    "def preprocess_data():\n",
    "    df = read_data('diabetes_dataset.csv')\n",
    "\n",
    "    df_cleaned = remove_outliers(df)\n",
    "\n",
    "    y_train: pd.Series\n",
    "    y_test: pd.Series\n",
    "    X_train, X_test, y_train, y_test = split_data(df_cleaned)\n",
    "\n",
    "    preprocessor = build_preprocessor(\n",
    "        NUM_COLS,\n",
    "        CAT_COLS,\n",
    "        ORDINAL_ENCODER_COLS,\n",
    "        ORDINAL_ENCODER_CATS,\n",
    "        ONEHOT_ENCODER_COLS,\n",
    "    )\n",
    "\n",
    "    X_train_pre_df, X_test_pre_df = process_features(preprocessor, X_train, X_test)\n",
    "\n",
    "    save_data(X_train_pre_df, X_test_pre_df, y_train, y_test)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edb1de5",
   "metadata": {},
   "source": [
    "The most important data cleaning decisions were the removal of records with medical inconsistencies, such as illogical blood pressure or cholesterol levels, to maintain physiological validity. Furthermore, ordinal encoding was used for hierarchical features to ensure that categorical data was transformed correctly for the modeling phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1241c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X_train.csv')\n",
    "X_test = pd.read_csv('X_test.csv')\n",
    "y_train = pd.read_csv('y_train.csv').values.ravel()\n",
    "y_test = pd.read_csv('y_test.csv').values.ravel()\n",
    "\n",
    "final_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabbc996",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7544a32b",
   "metadata": {},
   "source": [
    "Before applying linear regression we check how highly corelated features such as: LDL cholesterol, HbA1, and waist-to-hip ratio imply the model.\n",
    "\n",
    "**EDA cite:**\n",
    "> Glucose postprandial and HbA1c are extremely highly correlated (r=0.93), as are total cholesterol and LDL cholesterol (r=0.91). Fasting glucose correlates strongly with both HbA1c (r=0.70) and postprandial glucose (r=0.59). BMI and waist-to-hip ratio also show high correlation (r=0.77)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8249e4be",
   "metadata": {},
   "source": [
    "Drop one column for a correlated pair - chosen randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d0bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "corelated_features = ['waist_to_hip_ratio', 'ldl_cholesterol', 'hba1c']\n",
    "X_noncor_train = X_train.drop(columns=corelated_features)\n",
    "X_noncor_test = X_test.drop(columns=corelated_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbc7086952d1ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION\n",
    "def linear_regression_model(X_train, y_train, X_test, y_test):\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "    rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "    r2_lr = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "    print(f'Linear Regression RMSE: {rmse_lr:.6f}')\n",
    "    print(f'Linear Regression R2: {r2_lr:.6f}')\n",
    "\n",
    "    final_results.append({'Model': 'Linear Regression', 'RMSE': rmse_lr, 'R2': r2_lr})\n",
    "\n",
    "    return lr_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4b8b2d",
   "metadata": {},
   "source": [
    "Check the RMSE and R2 between two approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee7885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Linear Regression with all features:')\n",
    "lr_before_drop = linear_regression_model(X_train, y_train, X_test, y_test)\n",
    "print('Linear Regression after dropping correlated features:')\n",
    "lr_after_drop = linear_regression_model(X_noncor_train, y_train, X_noncor_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5811eff",
   "metadata": {},
   "source": [
    "Compare coeficients of those two models, I check coefs on features that where the pair for the removed ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf988435",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_check = ['glucose_postprandial', 'cholesterol_total', 'bmi']\n",
    "print('Coef before dropping correlated features:')\n",
    "for feature in features_to_check:\n",
    "    coef = lr_after_drop.coef_[list(X_noncor_train.columns).index(feature)]\n",
    "    print(f'{feature}: {coef}')\n",
    "print('\\nCoef after dropping correlated features:')\n",
    "for feature in features_to_check:\n",
    "    coef = lr_before_drop.coef_[list(X_train.columns).index(feature)]\n",
    "    print(f'{feature}: {coef}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6dd316",
   "metadata": {},
   "source": [
    "Dropping highly correlated variables (e.g., hba1c, ldl_cholesterol) did not decrease predictive accuracy, confirming that the dataset contained significant information redundancy. However, this step was essential for model interpretability; removing multicollinearity \"unmasked\" the true influence of key predictors, such as glucose_postprandial, which saw its coefficient increase tenfold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1d9cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = lr_after_drop\n",
    "X_train = X_noncor_train\n",
    "X_test = X_noncor_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131fa3dc",
   "metadata": {},
   "source": [
    "We picked the model trained on the dataset without corelated features as the linear regression model for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd7f5a7e51736b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RIDGE\n",
    "ridge_model = Ridge(alpha=1.0, random_state=42)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_ridge = ridge_model.predict(X_test)\n",
    "\n",
    "rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "print(f'Ridge Regression RMSE: {rmse_ridge:.6f}')\n",
    "print(f'Ridge Regression R2: {r2_ridge:.6f}')\n",
    "\n",
    "final_results.append({'Model': 'Ridge', 'RMSE': rmse_ridge, 'R2': r2_ridge})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936db132",
   "metadata": {},
   "source": [
    "Ridge regression achieved the same performance as the linear model with an R2 of 0.9935. This shows that the data has a strong linear structure and that L2 regularization successfully maintains accuracy while protecting the model from extreme coefficient values caused by correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8801ebc6fef26e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASSO\n",
    "lasso_model = Lasso(alpha=0.1, random_state=42)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lasso = lasso_model.predict(X_test)\n",
    "rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "print(f'Lasso RMSE: {rmse_lasso:.6f}')\n",
    "print(f'Lasso R2: {r2_lasso:.6f}')\n",
    "final_results.append({'Model': 'Lasso', 'RMSE': rmse_lasso, 'R2': r2_lasso})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca57c3c",
   "metadata": {},
   "source": [
    "Lasso regression has a slightly higher error than Ridge because it removed redundant features to simplify the model. The performance is still very high (R2 = 0.992), showing that we can identify the most important risk factors without losing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bb9a0ac24bf28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBOOST\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=200, learning_rate=0.05, max_depth=6, random_state=42, n_jobs=-1\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f'XGBoost RMSE: {rmse_xgb:.4f}')\n",
    "print(f'XGBoost R2: {r2_xgb:.6f}')\n",
    "final_results.append({'Model': 'XGBoost', 'RMSE': rmse_xgb, 'R2': r2_xgb})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230654de",
   "metadata": {},
   "source": [
    "XGBoost performed the best with the lowest RMSE (0.3069) and the highest R2 (0.9988). This shows that tree-based models are better at capturing complex non-linear patterns in the data compared to linear models. XGBoost is also naturally resistant to highly correlated features, allowing it to achieve better accuracy than linear regresion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917818991e3d758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUNING\n",
    "# Ridge\n",
    "linear_params = {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}\n",
    "\n",
    "ridge_grid = GridSearchCV(\n",
    "    Ridge(random_state=42), linear_params, scoring='neg_root_mean_squared_error', cv=5\n",
    ")\n",
    "ridge_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best Ridge Alpha: {ridge_grid.best_params_}')\n",
    "print(f'Best Ridge RMSE: {-ridge_grid.best_score_:.4f}')\n",
    "\n",
    "# Lasso\n",
    "lasso_grid = GridSearchCV(\n",
    "    Lasso(random_state=42), linear_params, scoring='neg_root_mean_squared_error', cv=5\n",
    ")\n",
    "lasso_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best Lasso Alpha: {lasso_grid.best_params_}')\n",
    "print(f'Best Lasso RMSE: {-lasso_grid.best_score_:.4f}')\n",
    "\n",
    "best_ridge = ridge_grid.best_estimator_\n",
    "best_lasso = lasso_grid.best_estimator_\n",
    "\n",
    "# xgboost\n",
    "xgb_params = {\n",
    "    'n_estimators': [500, 1000, 2000],\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'min_child_weight': [5, 7, 10, 15],\n",
    "    'learning_rate': [0.005, 0.01, 0.02, 0.05],\n",
    "    'subsample': [0.6, 0.7, 0.8],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8],\n",
    "    'reg_alpha': [0, 0.1, 1, 5],\n",
    "    'reg_lambda': [1, 2, 5],\n",
    "}\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    XGBRegressor(random_state=42, n_jobs=-1),\n",
    "    param_distributions=xgb_params,\n",
    "    n_iter=50,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "xgb_search.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best XGBoost params: {xgb_search.best_params_}')\n",
    "print(f'Best XGBoost RMSE: {-xgb_search.best_score_:.4f}')\n",
    "\n",
    "best_xgb = xgb_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f5db53",
   "metadata": {},
   "source": [
    "The tuned XGBoost model is the superior solution for this dataset, achieving significantly lower $RMSE$ (0.17) compared to linear models ($RMSE \\approx 0.73$). The near-perfect metrics suggest that while the data is likely synthetic, the models successfully identified the underlying deterministic patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d2ad9b2d09556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARISON\n",
    "feature_names = X_train.columns\n",
    "fig, axes = plt.subplots(4, 1, figsize=(24, 32))\n",
    "\n",
    "# 1. LINEAR REGRESSION\n",
    "df_lr = pd.DataFrame({'Feature': feature_names, 'Value': lr_model.coef_})\n",
    "df_lr = df_lr.reindex(df_lr['Value'].abs().sort_values(ascending=False).index).head(10)\n",
    "\n",
    "sns.barplot(\n",
    "    data=df_lr, x='Value', y='Feature', ax=axes[0], hue='Feature', palette='coolwarm'\n",
    ")\n",
    "axes[0].set_title(\n",
    "    'Baseline Linear Regression (No Regularization)', fontsize=16, fontweight='bold'\n",
    ")\n",
    "axes[0].axvline(0, color='black', linestyle='--', linewidth=1)\n",
    "axes[0].set_xlabel('Coefficient Value')\n",
    "\n",
    "# 2. RIDGE\n",
    "df_ridge = pd.DataFrame({'Feature': feature_names, 'Value': best_ridge.coef_})\n",
    "df_ridge = df_ridge.reindex(\n",
    "    df_ridge['Value'].abs().sort_values(ascending=False).index\n",
    ").head(10)\n",
    "\n",
    "sns.barplot(data=df_ridge, x='Value', y='Feature', ax=axes[1], hue='Feature')\n",
    "axes[1].set_title('Tuned Ridge', fontsize=14, fontweight='bold')\n",
    "axes[1].axvline(0, color='black', linestyle='--', linewidth=1)\n",
    "axes[1].set_xlabel('Importance')\n",
    "\n",
    "# 3. LASSO\n",
    "df_lasso = pd.DataFrame({'Feature': feature_names, 'Value': best_lasso.coef_})\n",
    "df_lasso = df_lasso.reindex(\n",
    "    df_lasso['Value'].abs().sort_values(ascending=False).index\n",
    ").head(10)\n",
    "\n",
    "sns.barplot(data=df_lasso, x='Value', y='Feature', ax=axes[2], hue='Feature')\n",
    "axes[2].set_title('Tuned Lasso', fontsize=14, fontweight='bold')\n",
    "axes[2].axvline(0, color='black', linestyle='--', linewidth=1)\n",
    "axes[2].set_xlabel('Importance')\n",
    "\n",
    "# 4. XGBOOST\n",
    "df_xgb = pd.DataFrame(\n",
    "    {'Feature': feature_names, 'Value': best_xgb.feature_importances_}\n",
    ")\n",
    "df_xgb = df_xgb.sort_values(by='Value', ascending=False).head(12)\n",
    "\n",
    "sns.barplot(data=df_xgb, x='Value', y='Feature', ax=axes[3], hue='Feature')\n",
    "axes[3].set_title('Tuned XGBoost', fontsize=14, fontweight='bold')\n",
    "axes[3].set_xlabel('Feature Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45a3363",
   "metadata": {},
   "source": [
    "The stability of the models is proven by the consistent ranking of features across Ridge, Lasso, and XGBoost, which all identified family history and age as primary predictors. Additionally, the logical direction of coefficients (e.g., negative for physical activity and positive for BMI) remains stable across the regularized models. While Ridge and Lasso achieved high accuracy ($R^2 \\approx 0.99$), their optimal regularization parameters were very low, making them perform similarly to standard regression. These minimal optimal alpha values indicate that the dataset has an exceptionally high signal-to-noise ratio and a nearly deterministic structure, where the relationships between variables are so clear that the models require almost no penalty to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee52be661aa1b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL TEST\n",
    "final_models = {\n",
    "    'Linear Regression': lr_model,\n",
    "    'Tuned Ridge': best_ridge,\n",
    "    'Tuned Lasso': best_lasso,\n",
    "    'Tuned XGBoost': best_xgb,\n",
    "}\n",
    "\n",
    "results_comparison = []\n",
    "\n",
    "for name, model in final_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    results_comparison.append({'Model': name, 'RMSE': rmse, 'MAE': mae, 'R2 Score': r2})\n",
    "\n",
    "comparison_df = pd.DataFrame(results_comparison).sort_values(by='RMSE')\n",
    "\n",
    "print(comparison_df)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "sns.barplot(\n",
    "    x='RMSE',\n",
    "    y='Model',\n",
    "    data=comparison_df,\n",
    "    palette='viridis',\n",
    "    ax=axes[0],\n",
    "    hue='Model',\n",
    "    legend=False,\n",
    ")\n",
    "axes[0].set_title('RMSE')\n",
    "axes[0].set_xlabel('Root Mean Squared Error')\n",
    "\n",
    "sns.barplot(\n",
    "    x='R2 Score',\n",
    "    y='Model',\n",
    "    data=comparison_df,\n",
    "    palette='magma',\n",
    "    ax=axes[1],\n",
    "    hue='Model',\n",
    "    legend=False,\n",
    ")\n",
    "axes[1].set_title('R2 Score')\n",
    "axes[1].set_xlabel('R2 Score')\n",
    "axes[1].set_xlim(comparison_df['R2 Score'].min() - 0.01, 1.005)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "top_features = [\n",
    "    'family_history_diabetes',\n",
    "    'age',\n",
    "    'physical_activity_minutes_per_week',\n",
    "    'bmi',\n",
    "    'glucose_fasting',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89ea6bc",
   "metadata": {},
   "source": [
    "The tuned XGBoost model achieved the best performance with an R2 of 0.9988, significantly outperforming linear models by capturing complex non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fb45f44d87fe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESIDUALS ANALYSIS\n",
    "y_pred_final = final_models['Tuned XGBoost'].predict(X_test)\n",
    "residuals = y_test - y_pred_final\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=y_test, y=residuals, alpha=0.5, color='purple')\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.title('Residual Plot')\n",
    "plt.xlabel('Real data')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, kde=True, color='purple', bins=30)\n",
    "plt.title('Errors distribution')\n",
    "plt.xlabel('Errors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aaebb5",
   "metadata": {},
   "source": [
    "The residual plot shows that the errors are randomly distributed around zero, which confirms the model captured the data patterns correctly. The normal distribution of errors proves that the model's predictions are unbiased and highly accurate. This final analysis validates the tuned XGBoost model as the most reliable choice for predicting diabetes risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bbedd9",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1b8138",
   "metadata": {},
   "source": [
    "The data was cleaned and preprocessed to address multicollinearity, confirming family history and age as the strongest diabetes risk factors. Using Lasso and Ridge regularization successfully stabilized feature coefficients and improved model interpretability without sacrificing accuracy. The tuned XGBoost model proved to be the most effective solution, achieving an R2 of 0.9988 and a highly precise RMSE of 0.1773.\n",
    "\n",
    "The near-perfect metrics suggest that the dataset is highly structured and likely synthetic. In a clinical setting, such accuracy is rare due to biological noise. However, for the purpose of this project, the data serves as an excellent benchmark to demonstrate the effectiveness of hyperparameter tuning and the removal of multicollinearity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med-project (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
