{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81737e6c",
   "metadata": {},
   "source": [
    "# Data Modeling - Diabetes Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5d704f",
   "metadata": {},
   "source": [
    "**Authors:**  \n",
    "Filip Kobus, Łukasz Jarzęcki, Paweł Skierkowski  \n",
    "**Date:** 23.01.25  \n",
    "**Team 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import Lasso, LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import sys, pathlib\n",
    "\n",
    "sys.path.append(str(pathlib.Path.cwd().parent / \"src\"))\n",
    "\n",
    "from med_project.config import PROCESSED_DATA_DIR\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdd3ebb523ddc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(PROCESSED_DATA_DIR / 'X_train.csv')\n",
    "X_test = pd.read_csv(PROCESSED_DATA_DIR / 'X_test.csv')\n",
    "y_train = pd.read_csv(PROCESSED_DATA_DIR / 'y_train.csv').values.ravel()\n",
    "y_test = pd.read_csv(PROCESSED_DATA_DIR / 'y_test.csv').values.ravel()\n",
    "\n",
    "final_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7544a32b",
   "metadata": {},
   "source": [
    "Before applying linear regression we check how highly corelated features such as: LDL cholesterol, HbA1, and waist-to-hip ratio imply the model.\n",
    "\n",
    "**EDA cite:**\n",
    "> Glucose postprandial and HbA1c are extremely highly correlated (r=0.93), as are total cholesterol and LDL cholesterol (r=0.91). Fasting glucose correlates strongly with both HbA1c (r=0.70) and postprandial glucose (r=0.59). BMI and waist-to-hip ratio also show high correlation (r=0.77)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8249e4be",
   "metadata": {},
   "source": [
    "Drop one column for a correlated pair - chosen randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d0bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "corelated_features = ['waist_to_hip_ratio', 'ldl_cholesterol', 'hba1c']\n",
    "X_noncor_train = X_train.drop(columns=corelated_features)\n",
    "X_noncor_test = X_test.drop(columns=corelated_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbc7086952d1ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION\n",
    "def linear_regression_model(X_train, y_train, X_test, y_test):\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "    rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "    r2_lr = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "    print(f'Linear Regression RMSE: {rmse_lr:.6f}')\n",
    "    print(f'Linear Regression R2: {r2_lr:.6f}')\n",
    "\n",
    "    final_results.append({'Model': 'Linear Regression', 'RMSE': rmse_lr, 'R2': r2_lr})\n",
    "\n",
    "    return lr_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4b8b2d",
   "metadata": {},
   "source": [
    "Check the RMSE and R2 between two approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee7885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear Regression with all features:\")\n",
    "lr_before_drop = linear_regression_model(X_train, y_train, X_test, y_test)\n",
    "print(\"Linear Regression after dropping correlated features:\")\n",
    "lr_after_drop = linear_regression_model(X_noncor_train, y_train, X_noncor_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5811eff",
   "metadata": {},
   "source": [
    "Compare coeficients of those two models, I check coefs on features that where the pair for the removed ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf988435",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_check = [\"glucose_postprandial\", \"cholesterol_total\", \"bmi\"]\n",
    "print(\"Coef before dropping correlated features:\")\n",
    "for feature in features_to_check:\n",
    "    coef = lr_after_drop.coef_[list(X_noncor_train.columns).index(feature)]\n",
    "    print(f\"{feature}: {coef}\")\n",
    "print(\"\\nCoef after dropping correlated features:\")\n",
    "for feature in features_to_check:\n",
    "    coef = lr_before_drop.coef_[list(X_train.columns).index(feature)]\n",
    "    print(f\"{feature}: {coef}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6dd316",
   "metadata": {},
   "source": [
    "Dropping highly correlated features did not change the model's accuracy because the information was redundant. However, it made the coefficients much higher (glucose_postprandial X10 higher) which proves that the original coefficients were masked by multicollinearity, leading to an underestimation of individual feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1d9cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = lr_after_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131fa3dc",
   "metadata": {},
   "source": [
    "We picked the model trained on the dataset without corelated as the linear regresion model for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd7f5a7e51736b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RIDGE\n",
    "ridge_model = Ridge(alpha=1.0, random_state=42)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_ridge = ridge_model.predict(X_test)\n",
    "\n",
    "rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "print(f'Ridge Regression RMSE: {rmse_ridge:.6f}')\n",
    "print(f'Ridge Regression R2: {r2_ridge:.6f}')\n",
    "\n",
    "final_results.append({'Model': 'Ridge', 'RMSE': rmse_ridge, 'R2': r2_ridge})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936db132",
   "metadata": {},
   "source": [
    "Ridge regression achieved the same performance as the linear model with an R2 of 0.9935. This shows that the data has a strong linear structure and that L2 regularization successfully maintains accuracy while protecting the model from extreme coefficient values caused by correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8801ebc6fef26e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASSO\n",
    "lasso_model = Lasso(alpha=0.1, random_state=42)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lasso = lasso_model.predict(X_test)\n",
    "rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "print(f'Lasso RMSE: {rmse_lasso:.6f}')\n",
    "print(f'Lasso R2: {r2_lasso:.6f}')\n",
    "final_results.append({'Model': 'Lasso', 'RMSE': rmse_lasso, 'R2': r2_lasso})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca57c3c",
   "metadata": {},
   "source": [
    "Lasso regression has a slightly higher error than Ridge because it removed redundant features to simplify the model. The performance is still very high (R2 = 0.992), showing that we can identify the most important risk factors without losing significant predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bb9a0ac24bf28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBOOST\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=200, learning_rate=0.05, max_depth=6, random_state=42, n_jobs=-1\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f'XGBoost RMSE: {rmse_xgb:.4f}')\n",
    "print(f'XGBoost R2: {r2_xgb:.6f}')\n",
    "final_results.append({'Model': 'XGBoost', 'RMSE': rmse_xgb, 'R2': r2_xgb})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230654de",
   "metadata": {},
   "source": [
    "XGBoost performed the best with the lowest RMSE (0.3050) and the highest R2 (0.9988). This shows that tree-based models are better at capturing complex non-linear patterns in the data compared to linear models. XGBoost is also naturally resistant to highly correlated features, allowing it to achieve superior accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917818991e3d758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUNING\n",
    "# Ridge\n",
    "linear_params = {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}\n",
    "\n",
    "ridge_grid = GridSearchCV(\n",
    "    Ridge(random_state=42), linear_params, scoring='neg_root_mean_squared_error', cv=5\n",
    ")\n",
    "ridge_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f'Najlepsze Ridge Alpha: {ridge_grid.best_params_}')\n",
    "print(f'Najlepsze Ridge RMSE: {-ridge_grid.best_score_:.4f}')\n",
    "\n",
    "# Lasso\n",
    "lasso_grid = GridSearchCV(\n",
    "    Lasso(random_state=42), linear_params, scoring='neg_root_mean_squared_error', cv=5\n",
    ")\n",
    "lasso_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f'Najlepsze Lasso Alpha: {lasso_grid.best_params_}')\n",
    "print(f'Najlepsze Lasso RMSE: {-lasso_grid.best_score_:.4f}')\n",
    "\n",
    "best_ridge = ridge_grid.best_estimator_\n",
    "best_lasso = lasso_grid.best_estimator_\n",
    "\n",
    "# xgboost\n",
    "xgb_params = {\n",
    "    'n_estimators': [500, 1000, 2000],\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'min_child_weight': [5, 7, 10, 15],\n",
    "    'learning_rate': [0.005, 0.01, 0.02, 0.05],\n",
    "    'subsample': [0.6, 0.7, 0.8],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8],\n",
    "    'reg_alpha': [0, 0.1, 1, 5],\n",
    "    'reg_lambda': [1, 2, 5],\n",
    "}\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    XGBRegressor(random_state=42, n_jobs=-1),\n",
    "    param_distributions=xgb_params,\n",
    "    n_iter=50,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "xgb_search.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best XGBoost params: {xgb_search.best_params_}')\n",
    "print(f'Best XGBoost RMSE: {-xgb_search.best_score_:.4f}')\n",
    "\n",
    "best_xgb = xgb_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f5db53",
   "metadata": {},
   "source": [
    "Hyperparameter tuning significantly improved the models, especially XGBoost, where the RMSE dropped from 0.30 to 0.17. For Ridge and Lasso, the best alpha values were very low, confirming that the linear relationships in the data are very strong. The tuned XGBoost model is the most precise and stable version of the regression for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d2ad9b2d09556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARISON\n",
    "feature_names = X_train.columns\n",
    "fig, axes = plt.subplots(3, 1, figsize=(24, 24))\n",
    "\n",
    "# Ridge\n",
    "df_ridge = pd.DataFrame({'Feature': feature_names, 'Value': best_ridge.coef_})\n",
    "\n",
    "df_ridge = df_ridge.reindex(\n",
    "    df_ridge['Value'].abs().sort_values(ascending=False).index\n",
    ").head(10)\n",
    "\n",
    "sns.barplot(data=df_ridge, x='Value', y='Feature', ax=axes[0], hue='Feature')\n",
    "axes[0].set_title('Tuned Ridge', fontsize=14, fontweight='bold')\n",
    "axes[0].axvline(0, color='black', linestyle='--', linewidth=1)\n",
    "axes[0].set_xlabel('Importance')\n",
    "\n",
    "# Lasso\n",
    "df_lasso = pd.DataFrame({'Feature': feature_names, 'Value': best_lasso.coef_})\n",
    "df_lasso = df_lasso.reindex(\n",
    "    df_lasso['Value'].abs().sort_values(ascending=False).index\n",
    ").head(10)\n",
    "\n",
    "sns.barplot(data=df_lasso, x='Value', y='Feature', ax=axes[1], hue='Feature')\n",
    "axes[1].set_title('Tuned Lasso', fontsize=14, fontweight='bold')\n",
    "axes[1].axvline(0, color='black', linestyle='--', linewidth=1)\n",
    "axes[1].set_xlabel('Importance')\n",
    "\n",
    "# xgboost\n",
    "df_xgb = pd.DataFrame(\n",
    "    {'Feature': feature_names, 'Value': best_xgb.feature_importances_}\n",
    ")\n",
    "df_xgb = df_xgb.sort_values(by='Value', ascending=False).head(12)\n",
    "\n",
    "sns.barplot(data=df_xgb, x='Value', y='Feature', ax=axes[2], hue='Feature')\n",
    "axes[2].set_title('Tuned XGBoost', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45a3363",
   "metadata": {},
   "source": [
    "1. Best Model: The tuned XGBoost model is the most accurate with an RMSE of 0.1773, significantly outperforming linear models.\n",
    "\n",
    "2. Feature Importance: All models consistently identified family history, age, BMI, and fasting glucose as the most critical predictors of diabetes risk.\n",
    "\n",
    "3. Regularization Impact: Using Lasso and Ridge ensured that the coefficients remained stable and the models generalized well to unseen data, even with high multicollinearity present in the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee52be661aa1b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL TEST\n",
    "final_models = {\n",
    "    'Linear Regression': lr_model,\n",
    "    'Tuned Ridge': best_ridge,\n",
    "    'Tuned Lasso': best_lasso,\n",
    "    'Tuned XGBoost': best_xgb,\n",
    "}\n",
    "\n",
    "results_comparison = []\n",
    "\n",
    "for name, model in final_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    results_comparison.append({'Model': name, 'RMSE': rmse, 'MAE': mae, 'R2 Score': r2})\n",
    "\n",
    "comparison_df = pd.DataFrame(results_comparison).sort_values(by='RMSE')\n",
    "\n",
    "print(comparison_df)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "sns.barplot(\n",
    "    x='RMSE',\n",
    "    y='Model',\n",
    "    data=comparison_df,\n",
    "    palette='viridis',\n",
    "    ax=axes[0],\n",
    "    hue='Model',\n",
    "    legend=False,\n",
    ")\n",
    "axes[0].set_title('RMSE')\n",
    "axes[0].set_xlabel('Root Mean Squared Error')\n",
    "\n",
    "sns.barplot(\n",
    "    x='R2 Score',\n",
    "    y='Model',\n",
    "    data=comparison_df,\n",
    "    palette='magma',\n",
    "    ax=axes[1],\n",
    "    hue='Model',\n",
    "    legend=False,\n",
    ")\n",
    "axes[1].set_title('R2 Score')\n",
    "axes[1].set_xlabel('R2 Score')\n",
    "axes[1].set_xlim(comparison_df['R2 Score'].min() - 0.01, 1.005)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "top_features = [\n",
    "    'family_history_diabetes',\n",
    "    'age',\n",
    "    'physical_activity_minutes_per_week',\n",
    "    'bmi',\n",
    "    'glucose_fasting',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89ea6bc",
   "metadata": {},
   "source": [
    "The tuned XGBoost model achieved the best performance with an R2 of 0.9988, significantly outperforming linear models by capturing complex non-linear relationships. Dropping redundant features and using Lasso regularization stabilized the coefficients for key factors like age, BMI, and glucose without decreasing overall accuracy. These results prove the model is highly precise and provides stable, reliable insights for identifying the most critical diabetes risk factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fb45f44d87fe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESIDUALS ANALYSIS\n",
    "y_pred_final = final_models['Tuned XGBoost'].predict(X_test)\n",
    "residuals = y_test - y_pred_final\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=y_test, y=residuals, alpha=0.5, color='purple')\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.title('Residual Plot')\n",
    "plt.xlabel('Real data')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, kde=True, color='purple', bins=30)\n",
    "plt.title('Errors distribution')\n",
    "plt.xlabel('Errors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aaebb5",
   "metadata": {},
   "source": [
    "The residual plot shows that the errors are randomly distributed around zero, which confirms the model captured the data patterns correctly. The normal distribution of errors proves that the model's predictions are unbiased and highly accurate. This final analysis validates the tuned XGBoost model as the most reliable choice for predicting diabetes risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bbedd9",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1b8138",
   "metadata": {},
   "source": [
    "The data was cleaned and preprocessed to address multicollinearity, confirming family history and age as the strongest diabetes risk factors. Using Lasso and Ridge regularization successfully stabilized feature coefficients and improved model interpretability without sacrificing accuracy. The tuned XGBoost model proved to be the most effective solution, achieving an R2 of 0.9988 and a highly precise RMSE of 0.1773."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med-project (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
